{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to gen more images wrt an input image and prompt.\n",
    "Source reference - Kaggle : Generation of images via Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from compel import Compel\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "# Download NLTK resources (you only need to do this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food_keywords': ['cheesy', 'pepperoni', 'pizza', 'top', 'pan']}\n"
     ]
    }
   ],
   "source": [
    "def foodKeywordMapping(caption=\"A cheesy pepperoni pizza sitting on top of a pan\"):\n",
    "    # Tokenize the caption into words\n",
    "    words = word_tokenize(caption)\n",
    "    # Perform part-of-speech tagging to identify nouns\n",
    "    tagged_words = pos_tag(words)\n",
    "    # Extract nouns related to food (NN: noun, NNPS: proper noun, NNS: plural noun)\n",
    "    food_keywords = [word for word, tag in tagged_words if tag in ['NN', 'NNPS', 'NNS']]\n",
    "    # Store the food keywords in a dictionary\n",
    "    food_dict = {'food_keywords': food_keywords}\n",
    "    return food_dict\n",
    "\n",
    "print(foodKeywordMapping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    batch_size = 32\n",
    "    return_sequences = 3\n",
    "    dataset = 'pets'\n",
    "    seed = 42\n",
    "    infer_steps = 100\n",
    "\n",
    "raw_images = \".\\data\\coco_food\\\\food_images\\\\all_food_train2017\\\\000000001059.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", \n",
    "                                                      torch_dtype=torch.float16,\n",
    "                                                      variant=\"fp16\",\n",
    "                                                      safety_checker = None,\n",
    "                                                      requires_safety_checker= False,\n",
    "                                                      use_safetensors=True).to(device)\n",
    "pipeline.enable_model_cpu_offload()\n",
    "compel_proc = Compel(tokenizer=pipeline.tokenizer, text_encoder=pipeline.text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kaggle version\n",
    "def get_input(idx, raw_images, df):\n",
    "    caption = df.loc[idx, 'best_caption']\n",
    "    label = class_mapping.get(raw_images[idx][1])\n",
    "    guidance = df.loc[idx, 'similarity']\n",
    "    guidance = -4 * guidance**2 + 2 * guidance + 1\n",
    "    prompt_embeds = compel_proc(f'(\"{caption}\", \"a picture of {label}\").blend(0.9, 1.5)')\n",
    "    image = raw_images[idx][0]\n",
    "    width, height = image.size\n",
    "    image = image.resize((width * 2, height * 2))\n",
    "    \n",
    "    if width > 650:\n",
    "        scale = int(width / 650) + 1\n",
    "        new_width = width // scale\n",
    "        new_height = height // scale\n",
    "        image = image.resize((new_width, new_height))\n",
    "    elif width < 400:\n",
    "        scale = 1.5\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "        image = image.resize((new_width, new_height))\n",
    "        \n",
    "    return {'prompt_embeds': prompt_embeds, \n",
    "            'num_images_per_prompt' : 3,\n",
    "            'num_inference_steps': args.infer_steps,\n",
    "            'image' : image, \n",
    "            'guidance':guidance,\n",
    "            'height': 224,\n",
    "            'width' : 224,\n",
    "            'noise' : 0.3\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My improvised version with label\n",
    "def get_input(idx, raw_images, df):\n",
    "    caption = \"A cheesy pepperoni pizza sitting on top of a pan\"\n",
    "    # Extract food keywords\n",
    "    food_keywords = foodKeywordMapping(caption)\n",
    "    # Convert the food keywords into a string separated by commas\n",
    "    label = ', '.join(food_keywords)\n",
    "    # Define the prompt for compel_proc with the label inserted\n",
    "    prompt = f'(\"{caption}\", \"a picture of {label}\").blend(0.9, 1.5)'\n",
    "    # Call compel_proc with the modified prompt\n",
    "    prompt_embeds = compel_proc(prompt)\n",
    "\n",
    "    guidance = df.loc[idx, 'similarity']\n",
    "    guidance = -4 * guidance**2 + 2 * guidance + 1\n",
    "    \n",
    "    image = raw_images[idx][0]\n",
    "    width, height = image.size\n",
    "    image = image.resize((width * 2, height * 2))\n",
    "    \n",
    "    if width > 650:\n",
    "        scale = int(width / 650) + 1\n",
    "        new_width = width // scale\n",
    "        new_height = height // scale\n",
    "        image = image.resize((new_width, new_height))\n",
    "    elif width < 400:\n",
    "        scale = 1.5\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "        image = image.resize((new_width, new_height))\n",
    "        \n",
    "    return {'prompt_embeds': prompt_embeds, \n",
    "            'num_images_per_prompt' : 3,\n",
    "            'num_inference_steps': args.infer_steps,\n",
    "            'image' : image, \n",
    "            'guidance':guidance,\n",
    "            'height': 224,\n",
    "            'width' : 224,\n",
    "            'noise' : 0.3\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, idx in enumerate(tqdm(range(0, 2))):\n",
    "    inputs = get_input(idx, raw_images, df)\n",
    "    image = pipeline(**inputs).images\n",
    "    for d in range(3):\n",
    "        image[d].save(f\"{args.dataset}_generated_{idx}_{d}.png\", format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig, axes = plt.subplots(1,4, figsize=(20, 10))\n",
    "\n",
    "axes[0].imshow(raw_images[0][0])\n",
    "axes[0].set_title('Original image')\n",
    "\n",
    "for i in range(3):\n",
    "    generated_image = Image.open(f\"/kaggle/working/pets_generated_0_{i}.png\")\n",
    "    axes[i+1].imshow(generated_image)\n",
    "    axes[i+1].set_title(f\"Generated image - {i}\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv3dr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
